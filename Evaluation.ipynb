{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "863fbc26-8139-475b-8148-c473a12bd48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/project/thesis_work/myenv/lib/python3.12/site-packages/', '', '/usr/local/jupyter/lib/python3.12/site-packages', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/lib/python3/dist-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "def ignore_user_installs(username):\n",
    "    ## avoid using user installs\n",
    "    user_install_path = '/scratch/' + username + '/python/lib/python3.12/site-packages'\n",
    "    if user_install_path in sys.path:\n",
    "        sys.path.remove(user_install_path)\n",
    "\n",
    "ignore_user_installs(\"lshaikh\")\n",
    "sys.path.insert(0,os.path.join('/project/thesis_work/myenv', 'lib/python3.12/site-packages/'))\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "374b43dc-3363-46be-809a-dfc630653178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5867f5042464987aad8e090be71aadf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "LOCAL_MODEL_DIR = \"/project/thesis_work/models/llama-2-7b-chat-hf\"  # Directory where the model is saved\n",
    "\n",
    "# Load tokenizer and model from local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIR, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LOCAL_MODEL_DIR,\n",
    "    torch_dtype=torch.float16,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "689870a9-6308-46e3-abf2-97f5492cfa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded concept vector: torch.Size([4096])\n",
      "\n",
      "--- Output ---\n",
      "Write a blunt reply to a friend who is crying over a breakup\n",
      "\n",
      "Breaking up can be difficult, but sometimes it's okay to be vulnerable and express our feelings. As a friend, you can offer support and comfort during this time. Here are some blunt replies to a friend who is crying over a breakup:\n",
      "\n",
      "1. \"I'm so sorry you're going through this. It's okay to feel sad and hurt. I'm here for you.\"\n",
      "2. \"Breakups can be tough, but they can also be an opportunity for growth and change. You're strong and capable, and you'll get through this.\"\n",
      "3. \"It's okay to feel upset and vulnerable after a breakup. You don't have to go through this alone, and I'm here to support you.\"\n",
      "4. \"Breakups can be a chance to learn and grow, and to find new ways of loving and being loved. You're not alone in this journey.\"\n",
      "5. \"It's okay to feel sad and emotional after a breakup. You're not alone, and I'm here to support you through this difficult time.\"\n",
      "\n",
      "Remember, being supportive and understanding can go a long way in helping a friend heal and move forward after a breakup. Be present, listen actively, and offer words of comfort and encouragement.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "LAYER_INDEX = 11\n",
    "STEER_STRENGTH = 9.0   # <0 removes concept, >0 amplifies it\n",
    "CAV_PATH = \"/project/thesis_work/gcav_results_empathy/cav_layer11.npy\"\n",
    "\n",
    "concept_vector = torch.tensor(np.load(CAV_PATH), dtype=torch.float16).to(model.device)\n",
    "\n",
    "print(\"✅ Loaded concept vector:\", concept_vector.shape)\n",
    "\n",
    "intervened_hidden_states = []\n",
    "original_hidden_states = []\n",
    "\n",
    "# ---------------- DEFINE HOOK ----------------\n",
    "def steering_hook(module, input, output):\n",
    "    # output shape: [batch, seq_len, hidden_dim]\n",
    "    steered = output + STEER_STRENGTH * concept_vector\n",
    "    original_hidden_states.append(output.detach().cpu())\n",
    "    intervened_hidden_states.append(steered.detach().cpu())\n",
    "    return steered\n",
    "\n",
    "# Register the hook on layer 11 (transformer block)\n",
    "handle = model.model.layers[LAYER_INDEX].register_forward_hook(steering_hook)\n",
    "\n",
    "# ---------------- GENERATE TEXT ----------------\n",
    "prompt = \"Write a blunt reply to a friend who is crying over a breakup\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=500,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "print(\"\\n--- Output ---\")\n",
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))\n",
    "\n",
    "# ---------------- CLEANUP ----------------\n",
    "handle.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "147646ce-1e18-43f9-b388-35f58e91c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states_intervened = intervened_hidden_states[0]\n",
    "hidden_states_original = original_hidden_states[0]\n",
    "H = hidden_states_intervened.reshape(-1, hidden_states_intervened.shape[-1]).numpy()\n",
    "H_base = hidden_states_original.reshape(-1, hidden_states_original.shape[-1]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8be4e2b-9c47-44ec-8150-0e63726b1e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[5.13334295e-01 8.49102245e-01 9.40141087e-01 6.58154662e-02\n",
      " 1.36719318e-03 2.65067898e-04 8.88378963e-03 5.23845826e-03\n",
      " 9.32407226e-02 2.08959432e-03 2.40854696e-03 9.00034055e-02\n",
      " 8.87082010e-04 2.07385568e-03 3.33777880e-01 8.84609386e-02\n",
      " 5.12706451e-01]\n",
      "Completeness Score: 0.7935\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "PROBE_PATH = \"/project/thesis_work/gcav_results_empathy/linear_probe_layer11.joblib\"\n",
    "\n",
    "probe = joblib.load(PROBE_PATH)\n",
    "\n",
    "# prediction probabilities on intervened hidden states\n",
    "probs_after = probe.predict_proba(H)[:, 1]\n",
    "\n",
    "print(probs_after)\n",
    "# for baseline (no intervention), repeat generation WITHOUT hook and collect H_base\n",
    "probs_before = probe.predict_proba(H_base)[:, 1]\n",
    "print(probs_before)\n",
    "\n",
    "completeness_score = np.mean(np.abs(probs_after - probs_before))\n",
    "\n",
    "print(f\"Completeness Score: {completeness_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "139b30c2-8768-43dd-9762-0b0ed0f9c56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'prompt': 'Write a blunt reply to a friend who is crying over a breakup', 'strength': np.float64(0.0), 'completeness': 0.0}, {'prompt': 'Write a blunt reply to a friend who is crying over a breakup', 'strength': np.float64(1.0), 'completeness': 0.704841060049755}, {'prompt': 'Write a blunt reply to a friend who is crying over a breakup', 'strength': np.float64(2.0), 'completeness': 0.7934956886147573}, {'prompt': 'Write a blunt reply to a friend who is crying over a breakup', 'strength': np.float64(3.0), 'completeness': 0.7935413922606478}, {'prompt': 'Write a blunt reply to a friend who is crying over a breakup', 'strength': np.float64(4.0), 'completeness': 0.7935414071187155}, {'prompt': 'Write a blunt reply to a friend who is crying over a breakup', 'strength': np.float64(5.0), 'completeness': 0.7935414071235497}, {'prompt': 'Write a blunt reply to a friend who is crying over a breakup', 'strength': np.float64(6.0), 'completeness': 0.7935414071235513}, {'prompt': 'Write a blunt reply to a friend who is crying over a breakup', 'strength': np.float64(7.0), 'completeness': 0.7935414071235513}, {'prompt': 'Write a blunt reply to a friend who is crying over a breakup', 'strength': np.float64(8.0), 'completeness': 0.7935414071235513}, {'prompt': 'Write a blunt reply to a friend who is crying over a breakup', 'strength': np.float64(9.0), 'completeness': 0.7935414071235513}, {'prompt': 'Write a blunt reply to a friend who is crying over a breakup', 'strength': np.float64(10.0), 'completeness': 0.7935414071235513}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "\n",
    "def evaluate_causal_effect(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    strength: float,\n",
    "    layer_idx: int,\n",
    "    target_probe_path: str,\n",
    "    # other_probe_paths: list,\n",
    "    cav_path: str = \"/project/thesis_work/gcav_results_empathy/cav_layer11.npy\",\n",
    "    max_new_tokens: int = 300,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the intervention for a single prompt + steering strength.\n",
    "    Computes completeness and selectivity using provided probes.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------- Load concept vector --------\n",
    "    concept_vector = torch.tensor(\n",
    "        np.load(cav_path), dtype=torch.float16\n",
    "    ).to(model.device)\n",
    "\n",
    "    original_hidden_states = []\n",
    "    intervened_hidden_states = []\n",
    "\n",
    "    # -------- Hook to apply steering --------\n",
    "    def steering_hook(module, input, output):\n",
    "        steered = output + strength * concept_vector\n",
    "        original_hidden_states.append(output.detach().cpu())\n",
    "        intervened_hidden_states.append(steered.detach().cpu())\n",
    "        return steered\n",
    "\n",
    "    # Register hook on transformer block\n",
    "    handle = model.model.layers[layer_idx].register_forward_hook(steering_hook)\n",
    "\n",
    "    # -------- Generate text with steering --------\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "    # Remove hook\n",
    "    handle.remove()\n",
    "\n",
    "    # Retrieve hidden states (take first forward pass)\n",
    "    H_inter = intervened_hidden_states[0]  # shape [1, seq, dim]\n",
    "    H_orig = original_hidden_states[0]\n",
    "\n",
    "    # Flatten sequence dimension\n",
    "    H_inter = H_inter.reshape(-1, H_inter.shape[-1]).numpy()\n",
    "    H_orig = H_orig.reshape(-1, H_orig.shape[-1]).numpy()\n",
    "\n",
    "    # -------- Load target probe (same as your code) --------\n",
    "    target_probe = joblib.load(target_probe_path)\n",
    "\n",
    "    probs_after = target_probe.predict_proba(H_inter)[:, 1]\n",
    "    probs_before = target_probe.predict_proba(H_orig)[:, 1]\n",
    "\n",
    "    completeness_score = float(np.mean(np.abs(probs_after - probs_before)))\n",
    "\n",
    "    # -------- Selectivity: compare with other probes --------\n",
    "    # selectivity_scores = []\n",
    "    # for probe_path in other_probe_paths:\n",
    "    #     probe = joblib.load(probe_path)\n",
    "    #     pa = probe.predict_proba(H_inter)[:, 1]\n",
    "    #     pb = probe.predict_proba(H_orig)[:, 1]\n",
    "    #     selectivity_scores.append(np.mean(np.abs(pa - pb)))\n",
    "\n",
    "    # # Selectivity = target_shift − average_other_shift\n",
    "    # selectivity_score = float(\n",
    "    #     completeness_score - np.mean(selectivity_scores)\n",
    "    # )\n",
    "\n",
    "    return {\n",
    "        \"completeness\": completeness_score,\n",
    "        # \"selectivity\": selectivity_score\n",
    "    }\n",
    "\n",
    "STEER_STRENGTHS = np.linspace(0, 10, 11)\n",
    "\n",
    "results = []   # list of dicts: {prompt, strength, completeness, selectivity}\n",
    "layer_idx = 11\n",
    "TEST_PROMPTS = [\"Write a blunt reply to a friend who is crying over a breakup\"]\n",
    "target_probe_path = \"/project/thesis_work/gcav_results_empathy/linear_probe_layer11.joblib\"\n",
    "\n",
    "for prompt in TEST_PROMPTS:\n",
    "    for strength in STEER_STRENGTHS:\n",
    "\n",
    "        scores = evaluate_causal_effect(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            strength=strength,\n",
    "            layer_idx=layer_idx,\n",
    "            target_probe_path=target_probe_path,\n",
    "            # other_probe_paths=other_probe_paths\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"strength\": strength,\n",
    "            \"completeness\": scores[\"completeness\"]\n",
    "            # \"selectivity\": scores[\"selectivity\"]\n",
    "        })\n",
    "\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28ab16c-f4c2-4698-841a-9e097ed65162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
